{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Welcome to Document Chatter AI App"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "from typing import List, Dict, Any, Tuple, Optional\n",
    "\n",
    "from langchain.document_loaders import PyPDFLoader, TextLoader\n",
    "from langchain_openai import OpenAIEmbeddings, OpenAI\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain.chains import ConversationalRetrievalChain\n",
    "import gradio as gr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Environment Setup\n",
    "This cell initializes the required libraries and sets up the environment variables, such as the OpenAI API key. Ensure you have the necessary packages installed before running this notebook.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retrieve OpenAI API key from .env file\n",
    "load_dotenv()\n",
    "\n",
    "# Initialize OpenAI API Key\n",
    "os.environ['OPENAI_API_KEY'] = os.getenv('OPENAI_API_KEY')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Documents\n",
    "This function handles loading and processing documents. It supports `.pdf` and `.txt` file formats and extracts their content for further processing. Unsupported file formats return an error.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_document(file_path: str) -> Tuple[Optional[List], Optional[str]]:\n",
    "    \"\"\"\n",
    "    Load and process a document from the given file path.\n",
    "\n",
    "    Parameters:\n",
    "        file_path (str): The path to the document file.\n",
    "\n",
    "    Returns:\n",
    "        tuple: A list of documents and an error message (if any).\n",
    "    \"\"\"\n",
    "    if file_path.endswith('.pdf'):\n",
    "        loader = PyPDFLoader(file_path)\n",
    "    elif file_path.endswith('.txt'):\n",
    "        loader = TextLoader(file_path)\n",
    "    else:\n",
    "        return None, \"Unsupported file format\"\n",
    "\n",
    "    documents = loader.load()\n",
    "    return documents, None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create a Vectorstore\n",
    "This function creates a FAISS vectorstore from the provided documents using OpenAI embeddings. The vectorstore is used for efficient retrieval during the question-answering process.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_vectorstore(documents: List) -> FAISS:\n",
    "    \"\"\"\n",
    "    Create a vectorstore from the provided documents using OpenAI embeddings.\n",
    "\n",
    "    Parameters:\n",
    "        documents (list): The list of documents to embed and store.\n",
    "\n",
    "    Returns:\n",
    "        FAISS: The vectorstore containing the embedded documents.\n",
    "    \"\"\"\n",
    "    embeddings = OpenAIEmbeddings()\n",
    "    vectorstore = FAISS.from_documents(documents, embeddings)\n",
    "    return vectorstore\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Upload and Process Documents\n",
    "This function handles file uploads, processes the document to extract its content, and initializes the QA chain for interaction with the document.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize global variables\n",
    "vectorstore: Optional[FAISS] = None\n",
    "qa_chain: Optional[ConversationalRetrievalChain] = None\n",
    "\n",
    "def upload_and_process(file: gr.File) -> str:\n",
    "    \"\"\"\n",
    "    Handle file upload, process the document, and initialize the QA chain.\n",
    "\n",
    "    Parameters:\n",
    "        file (gr.File): The uploaded file to process.\n",
    "\n",
    "    Returns:\n",
    "        str: Status message indicating success or failure.\n",
    "    \"\"\"\n",
    "    global vectorstore, qa_chain\n",
    "\n",
    "    documents, error = load_document(file.name)\n",
    "    if error:\n",
    "        return \"Error: \" + error\n",
    "\n",
    "    vectorstore = create_vectorstore(documents)\n",
    "    llm = OpenAI(temperature=0.5)\n",
    "    qa_chain = ConversationalRetrievalChain.from_llm(llm, vectorstore.as_retriever())\n",
    "\n",
    "    return \"Document processed and ready for chat!\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chat with Document\n",
    "This function enables users to query the uploaded document interactively. It maintains chat history to provide contextual responses.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def chat_with_document(query: str, history: List[Dict[str, str]]) -> List[Dict[str, str]]:\n",
    "    \"\"\"\n",
    "    Handle user queries by interacting with the uploaded document.\n",
    "\n",
    "    Parameters:\n",
    "        query (str): The user's query.\n",
    "        history (list): The chat history to maintain context.\n",
    "\n",
    "    Returns:\n",
    "        list: Updated chat history including the assistant's response.\n",
    "    \"\"\"\n",
    "    if not vectorstore or not qa_chain:\n",
    "        history.append({\"role\": \"assistant\", \"content\": \"Please upload and process a document first.\"})\n",
    "        return history\n",
    "\n",
    "    # Convert history into the format expected by ConversationalRetrievalChain\n",
    "    formatted_history = [(entry[\"role\"], entry[\"content\"]) for entry in history]\n",
    "\n",
    "    inputs = {\"question\": query, \"chat_history\": formatted_history}\n",
    "    result = qa_chain.invoke(inputs)\n",
    "\n",
    "    # Append the query and the response to the history\n",
    "    history.append({\"role\": \"user\", \"content\": query})\n",
    "    history.append({\"role\": \"assistant\", \"content\": result['answer']})\n",
    "\n",
    "    return history\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gradio UI Setup\n",
    "This cell sets up the Gradio interface for the application. It includes:\n",
    "- File upload for document processing.\n",
    "- A text input field for user queries.\n",
    "- A chatbot interface to display chat history and responses.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "with gr.Blocks() as demo:\n",
    "    gr.Markdown(\"# DocuChat: Interact with Your Documents\")\n",
    "\n",
    "    with gr.Row():\n",
    "        file_input = gr.File(label=\"Upload Document\")\n",
    "        upload_button = gr.Button(\"Process Document\")\n",
    "\n",
    "    status_output = gr.Textbox(label=\"Status\", interactive=False)\n",
    "\n",
    "    with gr.Row():\n",
    "        query_input = gr.Textbox(label=\"Your Question\")\n",
    "        submit_button = gr.Button(\"Ask\")\n",
    "\n",
    "    chat_history = gr.Chatbot(label=\"Chat History\", type=\"messages\")\n",
    "\n",
    "    # File upload and processing\n",
    "    upload_button.click(upload_and_process, inputs=file_input, outputs=status_output)\n",
    "\n",
    "    # Chat functionality\n",
    "    submit_button.click(chat_with_document, inputs=[query_input, chat_history], outputs=[chat_history])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Launch the Application\n",
    "This cell launches the Gradio application, allowing users to upload documents and interact with them through a chat interface.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Running on local URL:  http://127.0.0.1:7860\n",
      "\n",
      "To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7860/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Run the app\n",
    "demo.launch()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llms",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
